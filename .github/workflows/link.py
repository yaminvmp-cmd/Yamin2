import urllib.request
from bs4 import BeautifulSoup
import ssl
import time
import os
from urllib.parse import urlparse
import sys

ssl._create_default_https_context = ssl._create_unverified_context

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Android 10; Mobile)'
}

# ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐ ржПржмржВ рждрж╛ржжрзЗрж░ рж▓рж┐ржВржХ
CATEGORIES = {
    "Bangla_Movie": "https://www.notunmovie.link/category/bangla-movie/",
    "Bangla_Natok": "https://www.notunmovie.link/category/bangla-natok/",
    "Bangla_Web_Series": "https://www.notunmovie.link/category/bangla-web-series/",
    "Kolkata_Movie": "https://www.notunmovie.link/tag/kolkata-movie/",
    "Bangla_Dubbing_Movie": "https://www.notunmovie.link/category/bangla-dubbing-movie/",
    "Bangla_Dubbing_Web_Series": "https://www.notunmovie.link/category/bangla-dubbing-web-series/",
    "Hindi_Movie": "https://www.notunmovie.link/category/hindi-movie/",
    "Hindi_Dubbed_Movie": "https://www.notunmovie.link/category/hindi-dubbed-movie/",
    "Hindi_Web_Series": "https://www.notunmovie.link/category/hindi-web-series/",
    "Bangla Hot Web Series Collection (18+)": "https://www.notunmovie.link/category/bangla-hot-web-series-collection/"
}

def read_existing_links(filename):
    """ржлрж╛ржЗрж▓ ржерзЗржХрзЗ ржмрж┐ржжрзНржпржорж╛ржи рж▓рж┐ржВржХ ржкржбрж╝рзЗ ржЖрж╕рзЗ"""
    if not os.path.exists(filename):
        return set()
    
    with open(filename, "r", encoding="utf-8") as f:
        links = set()
        for line in f:
            line = line.strip()
            # рж╢рзБржзрзБржорж╛рждрзНрж░ рж▓рж┐ржВржХ рж▓рж╛ржЗржи ржирж┐ржмрзЗ (ржХржорзЗржирзНржЯ ржмрж╛ рж╣рзЗржбрж╛рж░ ржирзЯ)
            if line and not line.startswith('#') and not line.startswith('=') and '://' in line:
                links.add(line)
    return links

def scrape_category(category_name, base_url):
    """ржПржХржЯрж┐ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐ рж╕рзНржХрзНрж░рзНржпрж╛ржк ржХрж░рзЗ"""
    print(f"\n{'='*60}")
    print(f"ЁЯОм {category_name} рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ рж╢рзБрж░рзБ...")
    print(f"ЁЯФЧ URL: {base_url}")
    print(f"{'='*60}")
    
    # рж╢рзБржн ржлрзЛрж▓рзНржбрж╛рж░рзЗрж░ ржнрж┐рждрж░рзЗ ржлрж╛ржЗрж▓ рж╕рзЗржн рж╣ржмрзЗ
    if not os.path.exists("shuvo"):
        os.makedirs("shuvo")
    
    filename = f"shuvo/{category_name}.txt"
    existing_links = read_existing_links(filename)
    all_links = existing_links.copy()
    new_links = set()
    
    page = 1
    empty_pages = 0
    max_empty_pages = 2  # 2ржЯрж┐ ржЦрж╛рж▓рж┐ ржкрзЗржЬ ржкрж╛ржУрзЯрж╛рж░ ржкрж░ ржерж╛ржоржмрзЗ
    
    while True:
        try:
            # URL рждрзИрж░рж┐
            if page == 1:
                url = base_url.rstrip('/') + "/"
            else:
                url = f"{base_url.rstrip('/')}/page/{page}/"
            
            print(f"ЁЯУД {category_name} - Page {page} рж╕рзНржХрзНржпрж╛ржи ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ...")
            
            # рж░рж┐ржХрзБрзЯрзЗрж╕рзНржЯ ржкрж╛ржарж╛ржирзЛ
            req = urllib.request.Request(url, headers=HEADERS)
            response = urllib.request.urlopen(req, timeout=30)
            
            # HTTP рж╕рзНржЯрзНржпрж╛ржЯрж╛рж╕ ржЪрзЗржХ
            if response.status != 200:
                print(f"тЪая╕П  Page {page}: HTTP Status {response.status}")
                break
            
            html = response.read()
            soup = BeautifulSoup(html, 'html.parser')
            
            found_this_page = 0
            
            # ржкрзНрж░ржержорзЗ h2, h3 ржЯрзНржпрж╛ржЧрзЗ рж▓рж┐ржВржХ ржЦрзБржБржЬрж┐
            for h in soup.find_all(['h2', 'h3']):
                a = h.find('a')
                if a and a.get('href'):
                    link = a['href'].strip()
                    if link and '://' in link:
                        # ржбрзБржкрзНрж▓рж┐ржХрзЗржЯ ржЪрзЗржХ
                        if link not in all_links and link not in new_links:
                            # ржПржХржЗ ржбрзЛржорзЗржЗржи ржЪрзЗржХ
                            if 'notunmovie.link' in link or 'movie' in link.lower() or 'natok' in link.lower():
                                new_links.add(link)
                                all_links.add(link)
                                found_this_page += 1
                                print(f"   тЬЕ ржирждрзБржи рж▓рж┐ржВржХ #{len(new_links):03d}")
            
            # рждрж╛рж░ржкрж░ article ржЯрзНржпрж╛ржЧ ржЪрзЗржХ
            for article in soup.find_all('article'):
                a = article.find('a')
                if a and a.get('href'):
                    link = a['href'].strip()
                    if link and '://' in link:
                        if link not in all_links and link not in new_links:
                            if 'notunmovie.link' in link or 'movie' in link.lower() or 'natok' in link.lower():
                                new_links.add(link)
                                all_links.add(link)
                                found_this_page += 1
                                print(f"   тЬЕ ржирждрзБржи рж▓рж┐ржВржХ #{len(new_links):03d}")
            
            # рждрж╛рж░ржкрж░ рж╕ржм a ржЯрзНржпрж╛ржЧ ржЪрзЗржХ
            if found_this_page == 0:
                for a in soup.find_all('a', href=True):
                    link = a['href'].strip()
                    if link and '://' in link:
                        if link not in all_links and link not in new_links:
                            if 'notunmovie.link' in link or 'movie' in link.lower() or 'natok' in link.lower() or 'series' in link.lower():
                                new_links.add(link)
                                all_links.add(link)
                                found_this_page += 1
                                print(f"   тЬЕ ржирждрзБржи рж▓рж┐ржВржХ #{len(new_links):03d}")
            
            # ржЦрж╛рж▓рж┐ ржкрзЗржЬ ржХрж╛ржЙржирзНржЯ
            if found_this_page == 0:
                empty_pages += 1
                print(f"   тД╣я╕П  ржПржЗ ржкрзЗржЬрзЗ ржирждрзБржи рж▓рж┐ржВржХ ржкрж╛ржУрзЯрж╛ ржпрж╛рзЯржирж┐ ({empty_pages}/{max_empty_pages})")
                
                if empty_pages >= max_empty_pages:
                    print(f"\nтЫФ {category_name} - {max_empty_pages}ржЯрж┐ ржЦрж╛рж▓рж┐ ржкрзЗржЬ ржкрж╛ржУрзЯрж╛рзЯ рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ ржмржирзНржз")
                    break
            else:
                empty_pages = 0  # ржирждрзБржи рж▓рж┐ржВржХ ржкрж╛ржУрзЯрж╛рзЯ рж░рж┐рж╕рзЗржЯ
            
            # рж╕рж╛ржорж╛ржирзНржп ржмрж┐рж░рждрж┐ ржжрзЗржЗ
            time.sleep(0.5)
            page += 1
            
        except urllib.error.HTTPError as e:
            if e.code == 404:
                print(f"\nтЫФ {category_name} - Page {page} ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐ (404)")
                break
            else:
                print(f"\nтЪая╕П  HTTP Error {e.code}: {e.reason}")
                break
                
        except urllib.error.URLError as e:
            print(f"\nтЪая╕П  URL Error: {e.reason}")
            break
            
        except Exception as e:
            print(f"\nтЭМ Error: {str(e)}")
            break
    
    # ржирждрзБржи рж▓рж┐ржВржХ рж╕рзЗржн ржХрж░рж╛
    if new_links:
        # ржЖржЧрзЗрж░ рж╕ржм рж▓рж┐ржВржХ ржкрзЬрзЗ ржирж┐ржЗ
        all_existing_links = list(read_existing_links(filename))
        
        # ржирждрзБржи ржлрж╛ржЗрж▓ рждрзИрж░рж┐ ржХрж░рж┐
        with open(filename, "w", encoding="utf-8") as f:
            # рж╣рзЗржбрж╛рж░ рж▓рж┐ржЦрж┐
            f.write(f"{'='*60}\n")
            f.write(f"# {category_name}\n")
            f.write(f"# рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ рждрж╛рж░рж┐ржЦ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# ржорзЛржЯ рж▓рж┐ржВржХ: {len(all_existing_links) + len(new_links)}\n")
            f.write(f"{'='*60}\n\n")
            
            # ржЖржЧрзЗрж░ рж▓рж┐ржВржХржЧрзБрж▓рзЛ рж▓рж┐ржЦрж┐
            for link in sorted(all_existing_links):
                f.write(link + "\n")
            
            # ржирждрзБржи рж▓рж┐ржВржХржЧрзБрж▓рзЛ рж▓рж┐ржЦрж┐
            for link in sorted(new_links):
                f.write(link + "\n")
        
        print(f"\nЁЯТ╛ {category_name}: {len(new_links)} ржЯрж┐ ржирждрзБржи рж▓рж┐ржВржХ ржпрзЛржЧ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ")
        print(f"ЁЯУК ржорзЛржЯ рж▓рж┐ржВржХ: {len(all_existing_links) + len(new_links)} тЖТ {filename}")
    else:
        print(f"\nЁЯУн {category_name}: ржХрзЛржи ржирждрзБржи рж▓рж┐ржВржХ ржкрж╛ржУрзЯрж╛ ржпрж╛рзЯржирж┐")
        print(f"ЁЯУК ржкрзВрж░рзНржмрзЗрж░ рж▓рж┐ржВржХ: {len(existing_links)} тЖТ {filename}")
    
    print(f"{'='*60}")
    return len(new_links)

def main():
    """ржорзЗржЗржи ржлрж╛ржВрж╢ржи - рж╕ржмржХрж┐ржЫрзБ ржЕржЯрзЛржорзЗржЯрж┐ржХ ржЪрж╛рж▓рж╛ржмрзЗ"""
    print("\n" + "="*60)
    print("ЁЯЪА ржирждрзБржи ржорзБржнрж┐ ржЕржЯрзЛржорзЗржЯрж┐ржХ рж╕рзНржХрзНрж░рзНржпрж╛ржкрж╛рж░")
    print("="*60)
    print(f"тП░ рж╢рзБрж░рзБ: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ЁЯУБ ржорзЛржЯ ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐: {len(CATEGORIES)}")
    print("="*60)
    
    # рж╢рзБржн ржлрзЛрж▓рзНржбрж╛рж░ рждрзИрж░рж┐
    if not os.path.exists("shuvo"):
        os.makedirs("shuvo")
        print("ЁЯУВ 'shuvo' ржлрзЛрж▓рзНржбрж╛рж░ рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ")
    
    total_new_links = 0
    category_count = 1
    
    for category_name, base_url in CATEGORIES.items():
        print(f"\n[{category_count}/{len(CATEGORIES)}]", end=" ")
        new_links_count = scrape_category(category_name, base_url)
        total_new_links += new_links_count
        category_count += 1
        
        # ржкрзНрж░рждрж┐ ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐ ржкрж░ рж╕рж╛ржорж╛ржирзНржп ржмрж┐рж░рждрж┐
        if category_count <= len(CATEGORIES):
            print(f"\nтП│ ржкрж░ржмрж░рзНрждрзА ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐рж░ ржЬржирзНржп ржкрзНрж░рж╕рзНрждрзБржд рж╣ржЪрзНржЫрж┐...")
            time.sleep(2)
    
    # ржлрж╛ржЗржирж╛рж▓ рж░рж┐ржкрзЛрж░рзНржЯ
    print("\n" + "="*60)
    print("тЬЕ рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ рж╕ржорзНржкрзВрж░рзНржг!")
    print("="*60)
    print(f"ЁЯУЕ рж╢рзЗрж╖ рж╕ржорзЯ: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ЁЯУИ ржорзЛржЯ ржирждрзБржи рж▓рж┐ржВржХ: {total_new_links}")
    print(f"ЁЯУБ ржорзЛржЯ ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐: {len(CATEGORIES)}")
    
    # ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐ ржЕржирзБржпрж╛рзЯрзА ржлрж▓рж╛ржлрж▓
    print("\nЁЯУК ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐ ржЕржирзБржпрж╛рзЯрзА ржлрж▓рж╛ржлрж▓:")
    print("-"*40)
    
    for category_name in CATEGORIES.keys():
        filename = f"shuvo/{category_name}.txt"
        if os.path.exists(filename):
            with open(filename, "r", encoding="utf-8") as f:
                lines = f.readlines()
                link_count = 0
                for line in lines:
                    if line.strip() and '://' in line and not line.startswith('#'):
                        link_count += 1
            print(f"ЁЯУД {category_name}: {link_count} рж▓рж┐ржВржХ")
        else:
            print(f"ЁЯУД {category_name}: 0 рж▓рж┐ржВржХ (ржлрж╛ржЗрж▓ ржирзЗржЗ)")
    
    print("-"*40)
    print(f"ЁЯОЙ ржкрзНрж░рзЛржЧрзНрж░рж╛ржо рж╢рзЗрж╖! 5 рж╕рзЗржХрзЗржирзНржб ржкрж░ ржЕржЯрзЛржорзЗржЯрж┐ржХ ржмржирзНржз рж╣ржмрзЗ...")
    print("="*60)
    
    # 5 рж╕рзЗржХрзЗржирзНржб ржЕржкрзЗржХрзНрж╖рж╛ ржХрж░рзЗ ржЕржЯрзЛржорзЗржЯрж┐ржХ ржмржирзНржз
    time.sleep(5)
    sys.exit(0)

# ржкрзНрж░рзЛржЧрзНрж░рж╛ржо рж╢рзБрж░рзБ
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nтЭМ ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзА ржжрзНржмрж╛рж░рж╛ ржмржирзНржз ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ!")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nЁЯФе ржЕржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд рждрзНрж░рзБржЯрж┐: {e}")
        sys.exit(1)
